\pdfoutput=1 % only if pdf/png/jpg images are used
\documentclass{JINST}


\title{New Approaches to Uniform Boosting}

\author{
Aleksandar Bukva, 
Vladimir Gligorov$^d$,
Alex Rogozhnikov$^{a,b}$\thanks{Corresponding author.}~ ,
Andrey Ustuzhanin$^b$ and
Mike Williams$^c$\\
\llap{$^a$}Lomonosov Moscow State University, Moscow, Russia\\
\llap{$^b$}Yandex LLC, Moscow, Russia\\
\llap{$^c$}Massachusetts Institute of Technology, Cambridge, MA, United States \\
\llap{$^d$}Organisation Européenne pour la Recherche Nucléaire (CERN), Geneva \\Switzerland 
E-mail: \email{alex.rogozhnikov@yandex.ru}}


\abstract{
% Abstract must be short enough to appear in this page together with title, authors, their addresses and keywords.
The use of multivariate classifiers has become commonplace in particle physics. 
Typically, a series of classifiers is trained rather than just one to enhance the performance; this is known as boosting. 

In some applications of high energy physics (i.e., amplitude analyses) 
classifiers should not only optimize some integrated figure of merit, but produce a uniform selection efficiency in a space of selected physical variables.
Recently uBoost technique of boosting was proposed which addresses this issue.

In this paper we introduce several approaches to measure the uniformity of efficiency and propose new boosting techniques.
}





% AMS packages:
\usepackage{amsmath, amsthm, amsfonts}


% Theorems
%-----------------------------------------------------------------

\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}

\def\RR{\mathbb{R}}
\def\ZZ{\mathbb{Z}}
\newcommand{\abs}[1]{\left\vert#1\right\vert}
\newcommand{\sgn}{\operatorname{sgn}}


%-----------------------------------------------------------------


\begin{document}
\maketitle


\section{Introduction}

Methods of machine learning are playing a important role in particles physics nowadays, in particular, multivariate analysis (MVA). 
Different classifiers like boosted decision trees (BDTs) and artificial neural networks (ANNs) are often used as an important step in analysis selection criteria. 

BDTs are now even used in software triggers [link]. The main point of boosting technique is training many simple classifiers and building a composition of their outputs.
Classifiers are trained one-after-another, their inputs are augmented in such a way that new classifier should target more at those events which were poorly classified by previous ones. The resulting classifier obtained by combining them often much more powerful, than individual ones.


In practice, however, there are some restrictions that trained classifier should meet (apart from having good classification quality, which is usually measured by some integrated FOM). For example, in an amplitude analysis obtaining a uniform efficiency in a multivariate space of physics variates, i.e., variates that are of physical interest, is often times more important than any integrated FOM based on the total amount of signal and background. Such analyses often have many variates in which a uniform efficiency is desired. A uniform efficiency reduces systematic uncertainties and helps maintain sensitivity to all hypotheses being tested.

The one approach that was already proposed is uBoost (uniform BOOSTing) --- a modification of AdaBoost algorithm [link]

Why do we need such algorithms:

\begin{enumerate}
	\item The result of classification is stable to check different hypotheses
	\item To get unbiased distribution of signal, this enables us to compute real mass of particle
	\item Not to get fake peak, thus not get a false discovery
\end{enumerate}

The features, along which we want the prediction to be flat, will be referred to as 
\textit{uniform variables}. In high energy physics these are usually masses.

\section{Uniformity Measurement}

In this section we come up with some approaches on how to measure uniformity of prediction. The typical way of 'checking' uniformity of prediction used by physicists 
is fitting the distribution of the events that were classified as signal (or background) over the mass (or some other variable).

This approach is hardly formalizable, and not automatable --- each time you should assume some kind of distribution. 
Ideally we want to have some easy-to-use out-of-the-box metrics like FOMs in machine learning (like area under the ROC, f1 or ).

\subsection{Ideal uniformity}

We start from the simplest case --- when we are fully satisfied by predictions of our classifier. Remember that output of 
classification is probabilities of each event being a signal and background event, only after we select some cut on probability we get classification.

Ideal uniformity of signal prediction means that whichever cut we select, the efficiency (part of signal event that passed the cut) doesn't depend on uniform variables: 
in every region of uniform variables space the part of signal events that passed the cut is the same.

In practice, of course, this never happens.

There is uniformity of predictions on signal and uniformity of efficiency on background, which can be defined one from another by swapping classes. 
In what follows in this section we are writing about the efficiency on signal events.

A good example of classifier that has close to ideal uniformity is classifier which returns a random probability in [0, 1] range. What a pity: in practice it's absolutely useless.

% The most significant drawback of the upcoming metrics is they are ill-defined if there are not too many events of the named class. 

\subsection{Some Restrictions on Uniformity Metrics}

There are some additional conditions which we expect metric to meet:
\begin{enumerate}
\item
shouldn't depend much on the number of events (i.e., if we randomly select half of the events, the metrics should roughly be the same)
\item
shouldn't depend on the weights renormalization: if we multiply all the weight by some arbitrary number, it shouldn't change at all.
\item 
depends only on the order of predictions, not the exact values of probabilities.
This is because we care about which events pass the cut and which don't, not about the exact values of predictions.

Example: correlation of prediction and mass doesn't satisfy this restriction.
\item
parameter stability: if it uses bins, changing the number of bins shouldn't affect the metrics value much, if it uses $k$-nearest neighbors, it should be stable to small deviations of $k$.
\end{enumerate}


\subsection{Standard Deviation of Efficiency on Bins (SDE)}

\def\bineff{\text{eff}_\text{bin}}
\def\binweight{\text{weight}_\text{bin}}
\def\globaleff{\text{eff}}
\def\SDE{\text{SDE}}
\def\bin{\text{bin}}


Let's split the space of uniform features into bins, the uniformity in these terms is following: when we select some probability cut, 
the part of signal events that passes the cut is equal in all bins. Assume we selected some cut, then we have global efficiency
\[
	\globaleff = \dfrac{
		\text{total weight of signal events that passed the cut}}
		{\text{total weight of signal events}}
\]

Efficiency in every bin is defined respectively, 
\[
	\bineff = \dfrac{
		\text{weight of signal events in bin that passed the cut}}
		{\text{weight of signal events in this bin}} 
\]

So, basically, what we want to have in our dreams:
\[
	\bineff = \text{global efficiency} \qquad \forall \; \text{bin}
\]


To measure how far we are from ideal situation we use standard deviation:
\[
	\sqrt{\sum_{\bin} \left( \bineff - \globaleff \right)^2  }
\]

What is bad in this formula that every bin has some impact in the result, which does not depend on how many events are there, so metrics becomes very unstable to deviations in bins with only few events. To cure this, we add weights to the bins (note that $\sum_\bin \binweight = 1$):
\[
	\binweight = \dfrac{\text{total weight of signal events in bin}}
		{\text{total weight of signal events}},
\]
so we have SDE formula:
\[
	\SDE(\globaleff) = 
	\sqrt{\sum_{\bin} \binweight \times \left(\bineff - \globaleff \right)^2}. 
\] 
In fact, the expression depends on the cut, but for cuts which produce equal efficiency, this is 


Finally we note that the weighted average of $\bineff$ is $\globaleff$:
\[
	\globaleff = < \bineff > =  \sum_{\bin} \binweight \times \bineff,
\]
and this is why the introduced metrics was named SDE --- this is a weighted standard deviations of array of bin efficiencies.

But this is how we measure the non-uniformity for only one fixed cut, to measure the overall non-flatness, we take several global efficiencies (for instance, [0.5, 0.6, 0.7, 0.8, 0.9], because in practice usually we are interested in cuts with high global efficiency) and use 
\[
	\SDE^2  =  \frac{1}{k} 
	\sum_{\globaleff \in [\globaleff_1 \dots \globaleff_k] }  
		\text{SDE}^2(\globaleff)
\]

Some other power $p \neq 2$ can be used as well, but $p=2$ is considered as the default value: 
\[
	\SDE^p(\globaleff) = 
	\sum_{\bin} \binweight \times \abs{\bineff - \globaleff}^p,
\qquad
	\SDE^p  =  \frac{1}{k} 
	\sum_{\globaleff \in [\globaleff_1 \dots \globaleff_k] }  
		\text{SDE}^p(\globaleff).
\]



\subsection{Theil Index of Efficiency}
\def\theil{\text{Theil}}

One more measure uses Theil Index frequently used to measure economic inequality:
\[
	\theil = \frac{1}{N} \sum_i \frac{x_i}{<x>} \ln{\frac{x_i}{<x>}}, 
		\qquad <x> = \frac{1}{N} \sum_i x_i
\]
In our case we have to alter formula a bit to take into account that different bins have different impact, thus the formula turns into
\[
	\theil(\globaleff) = \sum_\bin \binweight \; \frac{\bineff}{\globaleff} \; \ln{\frac{\bineff}{\globaleff}}
\]

TODO how to combine Theil for different global efficiencies?
\[
	\theil = ??? \text{from} \theil(\globaleff)
\]

\subsection{Distribution Similarity Approach}
\label{sec:similarity}

Let's start from reformulation of what is uniform predictions in signal. First we split all signal events into some bins in uniform variables. There is some empirical distribution $F_\bin$ of predictions in each bin. Ideal uniformity means that all the distributions $F_\bin$ are equal and hence equal to the global distribution $F(x)$. 

To 'measure' non-flatness we can use some distribution distance, like Kolmogorov-Smirnov:
\[
	 \sum_{\bin} \binweight \max_x \abs{F_{\bin}(x) - F(x)},
\]
but Cram\'er--von Mises similarity is more informative (usually $p=2$ is used):
\[
	 \sum_{\bin} \binweight \int \abs{F_{\bin}(x) - F(x)}^p dF(x),
\]

The good point is we don't need to select some global efficiencies like in the other metrics.


\subsection{Connection Between SDE and Distance Similarity Approach}

SDE and DSA based on Cram\'er--von Mises similarity can be shown to have connection.
Let's consider the SDE with global efficiencies $= [1/N, 2/N, \dots, N/N]$. In the limit $N \to \infty$
\[
	\lim_{N \to \infty} \SDE^2 = 
	\lim_{N \to \infty} \frac{1}{N} \sum_{\globaleff}\SDE^2(\globaleff) = 
	\int_0^1 \SDE^2(\globaleff) d\, \globaleff = 
	\int_0^1 \sum_{\bin} \binweight \abs{\bineff - \globaleff}^2 d\, \globaleff
\]

From the other side, we can write the expression for similarity-based measure (for $p=2$) 
\[
	\sum_{\bin} \binweight \int \abs{F_\text{bin}(x) - F(x)}^2 dF(x) =
	\int \sum_{\bin} \binweight \abs{F_\text{bin}(x) - F(x)}^2 dF(x) 
\] The hard thing now is to believe this is literally the same and these two expressions are equal.


\subsection{Knn-based modifications}

\def\knni{\text{knn}(i)}
\def\effknni{\text{eff}_{\knni}}
\def\weightknni{\text{weight}_{\knni}}
\def\Fknn{F_{\knni}}

\def\knnSDE{\text{knnSDE}}

Though operating with bins is usually both simple and very efficient, 
in many cases it is hard to find optimal size of bins in the space of uniform variables (specifically in the case of more than two dimensions).
One more situation when bins-based approach fails, is when we have too few events to obtain a good statistics at least in several bins.

In these cases we can switch to $k$-nearest neighbors: for each signal event we find $k$ nearest signal events (including the event itself) in the space of uniform variables. Now we can compute the efficiency $\effknni$, empirical distribution $\Fknn$ of nearest neighbors. 
The weights for $\knni$ are proportional to the total weight of events in $\knni$:
\[
	\weightknni = \alpha \sum_{j \in \knni} w_j, \qquad \alpha^{-1} = \sum_i \sum_{j \in \knni} w_j,
\]
so again weights are normed to 1: $\sum_{i} \knni = 1$. 

Now we are ready to write knn version of SDE:
\[
	\knnSDE^2(\globaleff)
		= \sum_{i \in \text{events}} \weightknni \abs{\effknni - \globaleff}^2
\]
\[
	\knnSDE^2 = \sum_{\globaleff \in [\globaleff_1, \dots \globaleff_k]}
		\knnSDE^2(\globaleff),
\]
knn version of Theil index of Efficiency
\[
	\text{knnTheil}(\globaleff) = \sum_{i \in \text{events}} \weightknni \; \frac{\effknni}{\globaleff} \; \ln{\frac{\effknni}{\globaleff}}
\]
\[
	\text{knnTheil} = ??? \text{knnTheil}(\globaleff)
\]
and knn version of similarity-based measure:

\[
	 \sum_{i \in \text{events}} \weightknni \int \abs{\Fknn(x) - F(x)}^p dF(x),
\]


$K$-nearest neighbors approach suffers from the other drawback: the impact of different events has very little connection with the weights, because some events are met in knn of other events much more frequently while the other.
This effect can be suppressed by dividing initial weight of the event by the number of times it is met in knn. 




\subsection{Advantages and Disadvantages of Different Metrics}

... TODO, here some plots with comparison, maybe timings ...


\section{Approaches Proposed}

\subsection{Mean Ada Boost}

This is a modification of AdaBoost algorithm. In AdaBoost one multiplies weights in such a way:
\[
	w_i = w_i \times \exp[-y_i \, \text{score}_i],
\]
to enlarge the weights of poorly classified events ($y_i$ is $+1$ for signal and $-1$ for background).

But now we use the mean of prediction of $k$ nearest neighbors (of the same class)
\[
	w_i = w_i \times \exp[-y_i \, \dfrac{1}{k} \sum_{j \in \knni} \text{score}_j]
\]
Thus boosting focuses not on the events that were poorly classified, but on the regions with poor classification.


\subsection{Gradient Boosting with AdaLoss Modification (knn-Ada)}

\def\score{\text{score}}
\def\knn{\text{knn}}
\def\FL{\text{FL}}

Gradient boosting on trees is widely used algorithm[link], it's built upon decision tree regressors with usage of some loss function. 

Let's start with examples. One of the popular losses used is AdaLoss:
\[
	\sum_{i \in \text{events}} w_i \times \exp [- \score_i \, y_i] 
\]
where $y_i$ is either +1 (for signal events) or -1 (for background events). Good classification supposes that signal events should have large positive scores, while background ones should have large negative.

The predictions of separate regressors are simply summed up to form a score:
\[
	\score_i = \sum_{r \in \text{regressors}} \text{pred}_r(i),
\]
which can be 'translated' into probabilities by logistic function.

So the goal of algorithm is now to minimize the loss function. At each stage it trains one more regressor, which should decrease the value of loss, the most vivid way is to train it on negative gradient of loss. In the case of AdaLoss this can be done pretty easy:
\[
	-\dfrac{\partial \, \text{AdaLoss}}{\partial \, \score_i} = w_i \, y_i \exp[- \score_i \, y_i],
\]
so it is positive for signal events, negative for background events and has larger modulus for the events which are poorly classified.

After a new tree was built, it's output is altered: for each leaf we can compute such a value, that the result will give the smallest possible value for loss.

...computation of optimal value goes here...

This loss function can be easily modified to take in account not only the score for individual elements, but also 'finds' regions with lower-than-average quality.

\[
	\text{knnAdaLoss} = \sum_{i \in events} \exp[-y_i \times \sum_{j \in \text{knn}(i)} \score_j],
\]
where the index $j$ goes over the $k$ nearest neighbors of $i$th event, which belongs to the same class (signal or background).

We can introduce a supplementary sparse matrix $A \in \RR^{N \times N}$ ($N$ is number of events), which is defined as 
\[
a_{ij} = \begin{cases} 
1, & j \in \knn(i), \text{ events $i$ and $j$ belong to the same class} \\
0, & \text{otherwise},
\end{cases}
\] so the loss can be written as
\[
	\text{knnAdaLoss} = \sum_i \exp [- y_i \sum_j a_{ij} \, \score_j ],
\]
it's negative gradient is easily computed:
\[
	-\dfrac{\partial \, \text{knnAdaLoss}} {\partial \, \score_k} = 
	 y_k \, \sum_i a_{ik} \exp [- y_i \sum_j a_{ij} \, \score_j ],
\]
from this we can see that new algorithm will pay more attention to the events, which has poorly classified neighbors (and neighbors of neighbors). The named loss targets to obtain uniformity in both signal and background.

And the post-correction can be handled by using second-order Newton-Raphson optimization step 

... TODO computation of optimal value goes here ...

One can note, that the matrix $A$ doesn't need to be necessarily square. One can introduce $M$ groups of events (which may intersect), each group consists of several events with close uniform variables (and close events). Then one introduces $A \in \RR^{M \times N}$:
\[
	a_{mj} = \begin{cases}
		1, & \text{event $j$ is in group $m$} \\
		0, & \text{otherwise}
	\end{cases}
\]

In particular, if we take $A$ to be identity matrix: $A = I$ (each event to be in it's own group), knnAdaLoss turns into a simple AdaLoss.



\subsection{Gradient Boosting with Flatness Loss (FL)}

Let's use the metrics introduces in the section \ref{sec:similarity}:
\[
	\sum_{\bin} \binweight \int \abs{F_{\bin}(x) - F(x)}^p dF(x),
\]
which is good as a measure, but due to the non-smoothness of $F(x)$ it's 
gradient is singular, so we use instead
\[
	\FL = \sum_{\bin} \binweight \int \abs{F_{\bin}(x) - F(x)}^p dx.
\]

So, the derivative looks like:
\[
	\dfrac{\partial} {\partial \, \score_i} \FL
	= \sum_{\bin} \binweight \frac{\partial }{ \partial \, \score_i} 
			\int \abs{F_\bin(x) - F(x)}^p dx
\]
Let $\bin(i)$ be a bin to which event $i$ belongs, then we can compute:
\def\binIweight{\text{weight}_\text{\bin(i)}}

% TODO sign 

\begin{multline*}
	- \dfrac{\partial} {\partial \, \score_i} \FL = 
		- \binIweight
		\frac{\partial }{ \partial \, \score_i} 
			\int \abs{F_{\bin(i)}(x) - F(x)}^p dx \cong \\
	\cong \binIweight \, p \,  \abs{F_{\bin(i)}(x) - F(x)}^{p-1} 
		\sgn[F_{\bin(i)}(x) - F(x)]
		\dfrac{w_i}{\binIweight}
		\Bigg|_{x=\score_i} = \\
	= 
		w_i \, p \,  \abs{F_{\bin(i)}(x) - F(x)}^{p-1}
		\sgn[F_{\bin(i)}(x) - F(x)]
		\Bigg|_{x=\score_i}
\end{multline*}

TODO give explanations here

The next thing we need to point is FL doesn't take into account the quality of predictions. So what we use in practice is linear combination of FlatnessLoss and AdaLoss:
\[
	\text{loss} = \FL + \alpha \, \text{AdaLoss}
\]

First one penalizes the non-uniformity, second one --- poor predictions.

\section{Dataset}

Here should be some description of $D \to hhh$ sample.

\section{Plots}

\section{Timings}

The main drawback of uBoost technique is it's high computational complexity: 
while simple AdaBoost trains $M$ trees, uBoost builds $100 \times M$ trees (contribution of other operations usually can be neglected). 

Presented in this paper classifiers are building $M$ trees, though there is more complicated boosting, it takes at each iteration ($k$ is number of neighbors, $N$ is number of events in training sample)

\begin{itemize}
	\item meanAdaBoost: $O(k \times N)$
	\item knnAdaLoss: $O(k \times N)$, for the arbitrary matrix $A$ it is 
	$O( \text{\#nonzero elements in the matrix})$
	\item FlatnessLoss: $O(N \ln N)$
\end{itemize}


\section{Summary}

\section{Source code}

The link on the repository ans links to final notebooks with results in the article.

\acknowledgments



\begin{thebibliography}{9}

\bibitem{bib1}
Authors,
\emph{Title},
\emph{J. Ref.} \textbf{vol} (year) page.

\bibitem{bib2}
A. Pietropaolo et al., \emph{DINS measurements on VESUVIO in the
    resonance detector configuration: proton mean kinetic energy in
    water}, \jinst{1}{2006}{P04001}.

\bibitem{bib3}
A.I. Harris,
\emph{Spectroscopy with multichannel correlation radiometers},
\href{http://dx.doi.org/10.1063/1.1898643}
{\emph{Rev.\ Sci.\ Instrum.} {\bf 76} (2005) 054503}
[\astroph{0504449}].

\bibitem{bib4}
G.F. Knoll, \emph{Radiation detection and measurements}, John Wiley
    and Sons, Inc., New York 2000.

\bibitem{bib5}
V. Dangendorf, \emph{Time-resolved fast-neutron imaging with a
pulse-counting image intensifier}, in proceedings of
\emph{International workshop on fast neutron detectors and
applications}, April, 3--6, 2006 University of Cape Town, South Africa
\pos{PoS(FNDA2006)008}.

\end{thebibliography}

\end{document}
