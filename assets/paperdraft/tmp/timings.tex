The main drawback of uBoost technique is it's high computational complexity: 
while simple AdaBoost trains $M$ trees, uBoost builds $100 \times M$ trees (contribution of other operations usually can be neglected). 

Presented in this paper classifiers are building $M$ trees, though there is more complicated boosting, it takes at each iteration ($k$ is number of neighbors, $N$ is number of events in training sample)

\begin{itemize}
	\item meanAdaBoost: $O(k \times N)$
	\item knnAdaLoss: $O(k \times N)$, for the arbitrary matrix $A$ it is 
	$O( \text{\#nonzero elements in the matrix})$
	\item FlatnessLoss: $O(N \ln N)$
\end{itemize}
