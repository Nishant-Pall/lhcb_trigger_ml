Methods of machine learning are playing an important role in particles physics nowadays, in particular, multivariate analysis (MVA). 
Different classifiers like boosted decision trees (BDTs) and artificial neural networks (ANNs) are often used as an important step in analysis selection criteria. 

BDTs are now even used in software triggers [link]. The main point of boosting technique is training many simple classifiers and building a composition of their outputs.
Classifiers are trained one-after-another, their inputs are augmented in such a way that new classifier should target more at those events which were poorly classified by previous ones. The resulting classifier obtained by combining them often much more powerful, than individual ones.


In practice, however, there are some restrictions that trained classifier should meet (apart from having good classification quality, which is usually measured by some integrated FOM). For example, in an amplitude analysis obtaining a uniform efficiency in a multivariate space of physics variates, i.e., variates that are of physical interest, is often times more important than any integrated FOM based on the total amount of signal and background. Such analyses often have many variates in which a uniform efficiency is desired. A uniform efficiency reduces systematic uncertainties and helps maintain sensitivity to all hypotheses being tested.

The one approach that was already proposed is uBoost (uniform BOOSTing) --- a modification of AdaBoost algorithm [link]

Why do we need such algorithms:

\begin{enumerate}
	\item The result of classification is stable to check different hypotheses
	\item To get unbiased distribution of signal, this enables us to compute real mass of particle
	\item Not to get fake peak, thus not get a false discovery
\end{enumerate}

The features, along which we want the prediction to be flat, will be referred to as 
\textit{uniform variables}. In high energy physics these are usually masses.
