{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#About\n",
      "The implementation of uBoost in IPython _will be_ here soon <br />\n",
      "Up to this moment some modification of sklearn gradient boosting is here <br />\n",
      "\n",
      "The procedure of fitting is overriden (weights of events are altered) to pay attention to regions with low efficiency\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# uncomment for debug puposes\n",
      "# from NotebookHelper import execute_notebook\n",
      "# execute_notebook(\"CommonUtils.ipynb\")\n",
      "\n",
      "import sklearn\n",
      "import numpy\n",
      "from sklearn.neighbors import NearestNeighbors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ComputeBDTCut(targetEfficiency, answers, predictionProbas):\n",
      "    \"\"\"Computes cut which gives targetEfficiency\n",
      "    * targetEfficiency from 0 to 1\n",
      "    * answers is an array of zeros and ones\n",
      "    * predictionProbas is prediction probabilites returned by BDT at some step\n",
      "    \"\"\"\n",
      "    \n",
      "    assert(len(answers) == len(predictionProbas)), \"different size\"\n",
      "    \n",
      "    indices = (answers == 1)\n",
      "    signalProbas = predictionProbas[indices, 1]\n",
      "    return numpy.percentile(signalProbas, 100 - targetEfficiency * 100)\n",
      "\n",
      "\n",
      "def TestComputeBDT(size = 1000):\n",
      "    \"\"\" Tests computeBDT function \"\"\"\n",
      "    answers = numpy.zeros(size) + 1\n",
      "    probas = numpy.random.rand(size, 2)\n",
      "    for efficiency in [0.2, 0.3, 0.5, 0.7, 0.8, 0.9]:\n",
      "        cut = ComputeBDTCut(efficiency, answers, probas)\n",
      "        assert  -0.01 < ComputeEfficiency(cut, answers, probas) - efficiency < 0.01, \"something is wrong\"\n",
      "    print \"compute BDT is ok\"\n",
      "    \n",
      "TestComputeBDT()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "compute BDT is ok\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ComputeWeightMultiplies(targetEfficiency, knnIndices, answers, predictionProbas, gammaCoefficient):\n",
      "    \"\"\" Returns something that we will multiply boosting weights by\n",
      "    The greater gamma coefficient, \n",
      "    the greater importance of deviation from efficiency (may be changed dynamically)\n",
      "    \"\"\"\n",
      "    globalCut = ComputeBDTCut(targetEfficiency, answers, predictionProbas)\n",
      "    localEfficiencies = ComputeLocalEfficiencies(globalCut, knnIndices, answers, predictionProbas)\n",
      "    result = numpy.zeros(len(knnIndices))\n",
      "    for i in range(len(knnIndices)):\n",
      "        if answers[i] == 0:\n",
      "            result[i] = 1\n",
      "        else:\n",
      "            result[i] = exp((targetEfficiency - localEfficiencies[i]) * gammaCoefficient )\n",
      "    return result\n",
      "\n",
      "# todo write test function"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.utils import array2d\n",
      "from sklearn.tree._tree import DTYPE\n",
      "from collections import defaultdict\n",
      "from sklearn.ensemble._gradient_boosting import predict_stage, predict_stages\n",
      "\n",
      "\n",
      "class uniformerBDT(GradientBoostingClassifier):\n",
      "    \"\"\"This class is analog of one BDT in uBoost\n",
      "    This class inherits from sklearn's GradientBoostingClassifier,\n",
      "    and the procedure of fitting is overriden\n",
      "    \n",
      "    The resulting classifier should have uniform efficiency at targetEfficiency \n",
      "        (this is called \\bar{\\varepsilon} in uBoost paper)\n",
      "    along the uniforminVariables\n",
      "    \"\"\"\n",
      "    def __init__(self, uniformVariables = None, targetEfficiency = 0.5, neighbours = 100, loss='deviance', \n",
      "                 learning_rate=0.1, n_estimators=100,\n",
      "                 subsample=1.0, min_samples_split=2, min_samples_leaf=1,\n",
      "                 max_depth=3, init=None, random_state=None,\n",
      "                 max_features=None, verbose=0,\n",
      "                 max_leaf_nodes=None, warm_start=False):\n",
      "\n",
      "        GradientBoostingClassifier.__init__(\n",
      "                 self, loss = loss, learning_rate = learning_rate, n_estimators = n_estimators,\n",
      "                 subsample = subsample, min_samples_split = min_samples_split, \n",
      "                 min_samples_leaf = min_samples_leaf,\n",
      "                 max_depth = max_depth, init = init, random_state = random_state,\n",
      "                 max_features = max_features, verbose = verbose,\n",
      "                 # should be needed in the upcoming sklearn releases\n",
      "                 # max_leaf_nodes = max_leaf_nodes, warm_start = warm_start\n",
      "                 )\n",
      "        \n",
      "        self.uniformVariables = uniformVariables\n",
      "        self.targetEfficiency = targetEfficiency\n",
      "        self.neighbours = neighbours\n",
      "        \n",
      "    # TODO split on test and train\n",
      "    def fit(self, X, y, monitor=None):\n",
      "        \"\"\"Fit the gradient boosting model.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like, shape = [n_samples, n_features]\n",
      "            Training vectors, where n_samples is the number of samples\n",
      "            and n_features is the number of features.\n",
      "\n",
      "        y : array-like, shape = [n_samples]\n",
      "            Target values (integers in classification, real numbers in\n",
      "            regression)\n",
      "            For classification, labels must correspond to classes\n",
      "            ``0, 1, ..., n_classes_-1``.\n",
      "\n",
      "        monitor : callable, optional\n",
      "            The monitor is called after each iteration with the current\n",
      "            iteration, a reference to the estimator and the local variables of\n",
      "            ``_fit_stages`` as keyword arguments ``callable(i, self,\n",
      "            locals())``. If the callable returns ``True`` the fitting procedure\n",
      "            is stopped. The monitor can be used for various things such as\n",
      "            computing held-out estimates, early stopping, model introspect, and\n",
      "            snapshoting.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        self : object\n",
      "            Returns self.\n",
      "        \"\"\"\n",
      "        if self.uniformVariables is None:\n",
      "            throw(\"uniformVariables must be set\")\n",
      "        self.debugdict = defaultdict(list)\n",
      "        # computing knn matrix\n",
      "        # see http://scikit-learn.org/stable/modules/neighbors.html for algorithms of knn-ing\n",
      "        signalIndices = numpy.where(y)[0]\n",
      "        uniformingFeatures = X.ix[signalIndices, self.uniformVariables]\n",
      "\n",
      "        neighbours = NearestNeighbors(n_neighbors = self.neighbours, algorithm='kd_tree').fit(uniformingFeatures)\n",
      "        _, knnSignalIndices = neighbours.kneighbors(uniformingFeatures)\n",
      "\n",
      "        knnIndices = numpy.zeros((len(X), self.neighbours), dtype=numpy.int32)\n",
      "        \n",
      "        for index, signalNeigh in zip(signalIndices, knnSignalIndices):\n",
      "            knnIndices[index,:] = signalIndices[signalNeigh]\n",
      "        self.knnIndices = knnIndices\n",
      "        \n",
      "        GradientBoostingClassifier.fit(self, X = X, y = y\n",
      "                                       # to be included in the upcoming releases\n",
      "                                       #, monitor = monitor\n",
      "                                       )\n",
      "        return self\n",
      "    \n",
      "    def currentPredictProba(self, X, i):\n",
      "        X =  array2d(X, dtype=DTYPE, order=\"C\")\n",
      "        score = self._init_decision_function(X)\n",
      "        predict_stages(self.estimators_[:i], X, self.learning_rate, score)\n",
      "        return self._score_to_proba(score)\n",
      "\n",
      "    \n",
      "    def _fit_stage(self, i, X, y, y_pred, sample_mask, \n",
      "                    # to be included in future sklearn releases\n",
      "                    # criterion, splitter, \n",
      "                    random_state):\n",
      "        \"\"\"Fit another stage of ``n_classes_`` trees to the boosting model. \"\"\"\n",
      "        loss = self.loss_\n",
      "        original_y = y\n",
      "        \n",
      "        # TODO delete\n",
      "        mass = X[:,0]\n",
      "        isSignal = y > 0.5\n",
      "        isBG = y < 0.5\n",
      "\n",
      "\n",
      "        for k in range(loss.K):\n",
      "            if loss.is_multi_class:\n",
      "                y = np.array(original_y == k, dtype=np.float64)\n",
      "\n",
      "            residual = loss.negative_gradient(y, y_pred, k=k)\n",
      "\n",
      "            # induce regression tree on residuals\n",
      "            tree = DecisionTreeRegressor(\n",
      "#                 criterion=criterion,\n",
      "#                 splitter=splitter,\n",
      "                max_depth=self.max_depth,\n",
      "                min_samples_split=self.min_samples_split,\n",
      "                min_samples_leaf=self.min_samples_leaf,\n",
      "                max_features=self.max_features,\n",
      "#                 max_leaf_nodes=self.max_leaf_nodes,\n",
      "                random_state=random_state)\n",
      "    \n",
      "\n",
      "            # \u0432\u043e\u0442 \u0437\u0434\u0435\u0441\u044c \u0438\u0434\u0435\u0442 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u0432 \u043f\u0435\u0440\u0435\u0432\u0437\u0432\u0435\u0448\u0438\u0432\u0430\u043d\u0438\u0438\n",
      "            # \u0432\u044b\u0441\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u043c \u0433\u043b\u043e\u0431\u0430\u043b\u044c\u043d\u044b\u0439 \u043a\u0430\u0442\n",
      "            # \u0435\u0441\u043b\u0438 bg - \u0442\u043e \u0435\u0434\u0438\u043d\u0438\u0446\u0430\n",
      "            # \u0435\u0441\u043b\u0438 \u0441\u0438\u0433\u043d\u0430\u043b - \u0442\u043e \u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0435\u0433\u043e knn, \u0432\u044b\u0441\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u043c \u043b\u043e\u043a\u0430\u043b\u044c\u043d\u0443\u044e \u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u044c \u0438 \u0434\u043e\u043c\u043d\u043e\u0436\u0430\u0435\u043c\n",
      "            sample_weight = numpy.zeros(len(X)) + 1\n",
      "\n",
      "            if self.subsample < 1.0:\n",
      "                sample_weight = sample_mask.astype(np.float64)\n",
      "\n",
      "#             Attention - changes in weight are done here\n",
      "            probas = self.currentPredictProba(X, i)\n",
      "            multipliers = ComputeWeightMultiplies(self.targetEfficiency, \\\n",
      "                    self.knnIndices, y, probas, 2 * sqrt(i))\n",
      "            if i > 2:\n",
      "                sample_weight = sample_weight * multipliers\n",
      "                \n",
      "            self.debugdict['probas'].append(probas)\n",
      "            self.debugdict['multipliers'].append(multipliers)\n",
      "            self.debugdict['weight'].append(sample_weight)\n",
      "            self.debugdict['residual'].append(residual)\n",
      "            \n",
      "                \n",
      "            # \u043a\u043e\u043d\u0435\u0446 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0439\n",
      "\n",
      "            tree.fit(X, residual,\n",
      "                     sample_weight=sample_weight, check_input=False)\n",
      "    \n",
      "            self.debugdict['predicts'].append(tree.predict(X))\n",
      "    \n",
      "\n",
      "\n",
      "            # update tree leaves\n",
      "#             Temporally disabled this procedure - because it doesn't take uniformity into account\n",
      "#             but it should be rewritten later\n",
      "#             loss.update_terminal_regions(tree.tree_, X, y, residual, y_pred,\n",
      "#                                          sample_mask, self.learning_rate, k=k)\n",
      "            self.debugdict['predictsAfter'].append(tree.predict(X))\n",
      "\n",
      "            # add tree to ensemble\n",
      "            self.estimators_[i, k] = tree\n",
      "            \n",
      "\n",
      "        return y_pred\n",
      "    \n",
      "    def get_params(self, deep = False):\n",
      "        result = GradientBoostingClassifier.get_params(self, deep)\n",
      "        result['uniformVariables'] = self.uniformVariables\n",
      "        result['targetEfficiency'] = self.targetEfficiency\n",
      "        result['neighbours'] = self.neighbours\n",
      "        return result\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# todo some simple test function"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}