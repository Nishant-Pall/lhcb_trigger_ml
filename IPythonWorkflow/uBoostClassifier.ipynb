{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import defaultdict\n",
      "import numpy as np\n",
      "import numpy\n",
      "from numpy.core.umath_tests import inner1d\n",
      "import pandas\n",
      "from sklearn.ensemble.weight_boosting import BaseWeightBoosting, ClassifierMixin, _samme_proba, AdaBoostClassifier\n",
      "from sklearn.neighbors import NearestNeighbors\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "import math\n",
      "\n",
      "\n",
      "def ComputeBDTCut(targetEfficiency, answers, predictionProbas):\n",
      "    \"\"\"Computes cut which gives targetEfficiency\n",
      "    * targetEfficiency from 0 to 1\n",
      "    * answers is an array of zeros and ones\n",
      "    * predictionProbas is prediction probabilites returned by BDT at some step\n",
      "    \"\"\"\n",
      "\n",
      "    assert(len(answers) == len(predictionProbas)), \"different size\"\n",
      "\n",
      "    indices = (answers == 1)\n",
      "    signalProbas = predictionProbas[indices, 1]\n",
      "    return numpy.percentile(signalProbas, 100 - targetEfficiency * 100)\n",
      "\n",
      "\n",
      "def ComputeLocalEfficiencies(globalCut, knnIndices, answers, predictionProbas):\n",
      "    assert len(answers) == len(predictionProbas), 'different size'\n",
      "    result = numpy.zeros(len(answers))\n",
      "    predictions = predictionProbas[:,1] > globalCut\n",
      "    for i in range(len(knnIndices)):\n",
      "        neighbours = knnIndices[i]\n",
      "        # TODO delete assert\n",
      "        if answers[i] == 1:\n",
      "            assert(numpy.all(answers[neighbours] == 1))\n",
      "        result[i] = numpy.sum( predictions[neighbours] ) * 1.0 / len(neighbours)\n",
      "    return result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class uBoostBDT(AdaBoostClassifier):\n",
      "    \"\"\"An AdaBoost classifier.\n",
      "\n",
      "    An AdaBoost [1] classifier is a meta-estimator that begins by fitting a\n",
      "    classifier on the original dataset and then fits additional copies of the\n",
      "    classifier on the same dataset but where the weights of incorrectly\n",
      "    classified instances are adjusted such that subsequent classifiers focus\n",
      "    more on difficult cases.\n",
      "\n",
      "    This class implements the algorithm known as AdaBoost-SAMME [2].\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    base_estimator : object, optional (default=DecisionTreeClassifier)\n",
      "        The base estimator from which the boosted ensemble is built.\n",
      "        Support for sample weighting is required, as well as proper `classes_`\n",
      "        and `n_classes_` attributes.\n",
      "\n",
      "    n_estimators : integer, optional (default=50)\n",
      "        The maximum number of estimators at which boosting is terminated.\n",
      "        In case of perfect fit, the learning procedure is stopped early.\n",
      "\n",
      "    learning_rate : float, optional (default=1.)\n",
      "        Learning rate shrinks the contribution of each classifier by\n",
      "        ``learning_rate``. There is a trade-off between ``learning_rate`` and\n",
      "        ``n_estimators``.\n",
      "\n",
      "    algorithm : {'SAMME', 'SAMME.R'}, optional (default='SAMME.R')\n",
      "        If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n",
      "        ``base_estimator`` must support calculation of class probabilities.\n",
      "        If 'SAMME' then use the SAMME discrete boosting algorithm.\n",
      "        The SAMME.R algorithm typically converges faster than SAMME,\n",
      "        achieving a lower test error with fewer boosting iterations.\n",
      "\n",
      "    random_state : int, RandomState instance or None, optional (default=None)\n",
      "        If int, random_state is the seed used by the random number generator;\n",
      "        If RandomState instance, random_state is the random number generator;\n",
      "        If None, the random number generator is the RandomState instance used\n",
      "        by `np.random`.\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    `estimators_` : list of classifiers\n",
      "        The collection of fitted sub-estimators.\n",
      "\n",
      "    `classes_` : array of shape = [n_classes]\n",
      "        The classes labels.\n",
      "\n",
      "    `n_classes_` : int\n",
      "        The number of classes.\n",
      "\n",
      "    `estimator_weights_` : array of floats\n",
      "        Weights for each estimator in the boosted ensemble.\n",
      "\n",
      "    `estimator_errors_` : array of floats\n",
      "        Classification error for each estimator in the boosted\n",
      "        ensemble.\n",
      "\n",
      "    `feature_importances_` : array of shape = [n_features]\n",
      "        The feature importances if supported by the ``base_estimator``.\n",
      "\n",
      "    See also\n",
      "    --------\n",
      "    AdaBoostRegressor, GradientBoostingClassifier, DecisionTreeClassifier\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n",
      "           on-Line Learning and an Application to Boosting\", 1995.\n",
      "\n",
      "    .. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n",
      "\n",
      "    \"\"\"\n",
      "    def __init__(self,\n",
      "                 uniformVariables,\n",
      "                 targetEfficiency = 0.5,\n",
      "                 neighbours = 100,\n",
      "                 base_estimator=None,\n",
      "                 n_estimators=50,\n",
      "                 learning_rate=1.,\n",
      "                 algorithm='SAMME.R',\n",
      "                 random_state=None):\n",
      "        AdaBoostClassifier.__init__(self,\n",
      "                 base_estimator=base_estimator,\n",
      "                 n_estimators=n_estimators,\n",
      "                 learning_rate=learning_rate,\n",
      "                 algorithm=algorithm,\n",
      "                 random_state=random_state)\n",
      "\n",
      "        self.uniformVariables = uniformVariables\n",
      "        self.targetEfficiency = targetEfficiency\n",
      "        self.neighbours = neighbours\n",
      "\n",
      "    def fit(self, X, y, sample_weight=None):\n",
      "        \"\"\"Build a boosted classifier from the training set (X, y).\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape = [n_samples, n_features]\n",
      "            The training input samples.\n",
      "\n",
      "        y : array-like of shape = [n_samples]\n",
      "            The target values (integers that correspond to classes).\n",
      "\n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights. If None, the sample weights are initialized to\n",
      "            ``1 / n_samples``.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        self : object\n",
      "            Returns self.\n",
      "        \"\"\"\n",
      "\n",
      "        if self.uniformVariables is None:\n",
      "            raise ValueError(\"uniformVariables must be set\")\n",
      "        self.debugdict = defaultdict(list)\n",
      "        # computing knn matrix\n",
      "        # see http://scikit-learn.org/stable/modules/neighbors.html for algorithms of knn-ing\n",
      "        signalIndices = numpy.where(y)[0]\n",
      "        uniformingFeatures = X.ix[signalIndices, self.uniformVariables]\n",
      "\n",
      "        neighbours = NearestNeighbors(n_neighbors = self.neighbours, algorithm='kd_tree').fit(uniformingFeatures)\n",
      "        _, knnSignalIndices = neighbours.kneighbors(uniformingFeatures)\n",
      "\n",
      "        knnIndices = numpy.zeros((len(X), self.neighbours), dtype=numpy.int32)\n",
      "\n",
      "        for index, signalNeigh in zip(signalIndices, knnSignalIndices):\n",
      "            knnIndices[index,:] = signalIndices[signalNeigh]\n",
      "        self.knnIndices = knnIndices\n",
      "\n",
      "        return AdaBoostClassifier.fit(self, X, y, sample_weight)\n",
      "\n",
      "\n",
      "\n",
      "    def _boost(self, iboost, X, y, sample_weight):\n",
      "        \"\"\"Implement a single boost.\n",
      "\n",
      "        Perform a single boost according to the real multi-class SAMME.R\n",
      "        algorithm or to the discrete SAMME algorithm and return the updated\n",
      "        sample weights.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        iboost : int\n",
      "            The index of the current boost iteration.\n",
      "\n",
      "        X : array-like of shape = [n_samples, n_features]\n",
      "            The training input samples.\n",
      "\n",
      "        y : array-like of shape = [n_samples]\n",
      "            The target values (integers that correspond to classes).\n",
      "\n",
      "        sample_weight : array-like of shape = [n_samples]\n",
      "            The current sample weights.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        sample_weight : array-like of shape = [n_samples] or None\n",
      "            The reweighted sample weights.\n",
      "            If None then boosting has terminated early.\n",
      "\n",
      "        estimator_weight : float\n",
      "            The weight for the current boost.\n",
      "            If None then boosting has terminated early.\n",
      "\n",
      "        estimator_error : float\n",
      "            The classification error for the current boost.\n",
      "            If None then boosting has terminated early.\n",
      "        \"\"\"\n",
      "        if self.algorithm == 'SAMME.R':\n",
      "            return self._boost_real(iboost, X, y, sample_weight)\n",
      "\n",
      "        else:  # elif self.algorithm == \"SAMME\":\n",
      "            return self._boost_discrete(iboost, X, y, sample_weight)\n",
      "\n",
      "    def _boost_real(self, iboost, X, y, sample_weight):\n",
      "        \"\"\"Implement a single boost using the SAMME.R real algorithm.\"\"\"\n",
      "        estimator = self._make_estimator()\n",
      "\n",
      "        try:\n",
      "            estimator.set_params(random_state=self.random_state)\n",
      "        except ValueError:\n",
      "            pass\n",
      "\n",
      "        estimator.fit(X, y, sample_weight=sample_weight)\n",
      "\n",
      "        y_predict_proba = estimator.predict_proba(X)\n",
      "\n",
      "        if iboost == 0:\n",
      "            self.classes_ = getattr(estimator, 'classes_', None)\n",
      "            self.n_classes_ = len(self.classes_)\n",
      "\n",
      "        y_predict = self.classes_.take(np.argmax(y_predict_proba, axis=1),\n",
      "                                       axis=0)\n",
      "\n",
      "        # Instances incorrectly classified\n",
      "        incorrect = y_predict != y\n",
      "\n",
      "        # Error fraction\n",
      "        estimator_error = np.mean(np.average(incorrect, weights=sample_weight, axis=0))\n",
      "\n",
      "        # Stop if classification is perfect\n",
      "        if estimator_error <= 0:\n",
      "            return sample_weight, 1., 0.\n",
      "\n",
      "        # Construct y coding as described in Zhu et al [2]:\n",
      "        #\n",
      "        #    y_k = 1 if c == k else -1 / (K - 1)\n",
      "        #\n",
      "        # where K == n_classes_ and c, k in [0, K) are indices along the second\n",
      "        # axis of the y coding with c being the index corresponding to the true\n",
      "        # class label.\n",
      "        n_classes = self.n_classes_\n",
      "        classes = self.classes_\n",
      "        y_codes = np.array([-1. / (n_classes - 1), 1.])\n",
      "        y_coding = y_codes.take(classes == y[:, np.newaxis])\n",
      "\n",
      "        # Displace zero probabilities so the log is defined.\n",
      "        # Also fix negative elements which may occur with\n",
      "        # negative sample weights.\n",
      "        y_predict_proba[y_predict_proba <= 0] = 1e-5\n",
      "\n",
      "        # Boost weight using multi-class AdaBoost SAMME.R alg\n",
      "        estimator_weight = (-1. * self.learning_rate\n",
      "                                * (((n_classes - 1.) / n_classes) *\n",
      "                                   inner1d(y_coding, np.log(y_predict_proba))))\n",
      "\n",
      "        # Only boost the weights if it will fit again\n",
      "        if not iboost == self.n_estimators - 1:\n",
      "            # Only boost positive weights\n",
      "            sample_weight *= np.exp(estimator_weight *\n",
      "                                    ((sample_weight > 0) |\n",
      "                                     (estimator_weight < 0)))\n",
      "            # here some uBoost-like code change\n",
      "            globalCut = ComputeBDTCut(self.targetEfficiency, y, y_predict_proba)\n",
      "            localEfficiencies = ComputeLocalEfficiencies(globalCut, self.knnIndices, y, y_predict_proba)\n",
      "            eprime = numpy.sum(sample_weight * abs(localEfficiencies - self.targetEfficiency))\n",
      "            beta = math.log((1.0-eprime) / eprime)\n",
      "            print 'iteration', iboost\n",
      "            print 'beta', beta, 'eprime', eprime\n",
      "            print 'sample_weight', sample_weight\n",
      "            print 'y', y\n",
      "            print 'weight_sum', numpy.sum(sample_weight)\n",
      "            print localEfficiencies\n",
      "            # weight is changed only for signal events\n",
      "            sample_weight *= np.exp((self.targetEfficiency - localEfficiencies) * (y > 0.5) * beta)\n",
      "            print 'sample_weight', sample_weight\n",
      "\n",
      "        return sample_weight, 1., estimator_error\n",
      "\n",
      "\n",
      "    def _boost_discrete(self, iboost, X, y, sample_weight):\n",
      "        \"\"\"Implement a single boost using the SAMME discrete algorithm.\"\"\"\n",
      "        raise NotImplementedError()\n",
      "        # estimator = self._make_estimator()\n",
      "        #\n",
      "        # try:\n",
      "        #     estimator.set_params(random_state=self.random_state)\n",
      "        # except ValueError:\n",
      "        #     pass\n",
      "        #\n",
      "        # estimator.fit(X, y, sample_weight=sample_weight)\n",
      "        #\n",
      "        # y_predict = estimator.predict(X)\n",
      "        #\n",
      "        # if iboost == 0:\n",
      "        #     self.classes_ = getattr(estimator, 'classes_', None)\n",
      "        #     self.n_classes_ = len(self.classes_)\n",
      "        #\n",
      "        # # Instances incorrectly classified\n",
      "        # incorrect = y_predict != y\n",
      "        #\n",
      "        # # Error fraction\n",
      "        # estimator_error = np.mean(\n",
      "        #     np.average(incorrect, weights=sample_weight, axis=0))\n",
      "        #\n",
      "        # # Stop if classification is perfect\n",
      "        # if estimator_error <= 0:\n",
      "        #     return sample_weight, 1., 0.\n",
      "        #\n",
      "        # n_classes = self.n_classes_\n",
      "        #\n",
      "        # # Stop if the error is at least as bad as random guessing\n",
      "        # if estimator_error >= 1. - (1. / n_classes):\n",
      "        #     self.estimators_.pop(-1)\n",
      "        #     return None, None, None\n",
      "        #\n",
      "        # # Boost weight using multi-class AdaBoost SAMME alg\n",
      "        # estimator_weight = self.learning_rate * (\n",
      "        #     np.log((1. - estimator_error) / estimator_error) +\n",
      "        #     np.log(n_classes - 1.))\n",
      "        #\n",
      "        # # Only boost the weights if I will fit again\n",
      "        # if not iboost == self.n_estimators - 1:\n",
      "        #     # Only boost positive weights\n",
      "        #     sample_weight *= np.exp(estimator_weight * incorrect *\n",
      "        #                             ((sample_weight > 0) |\n",
      "        #                              (estimator_weight < 0)))\n",
      "        #\n",
      "        # return sample_weight, estimator_weight, estimator_error\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# from sklearn.tree.tree import DecisionTreeClassifier\n",
      "# baseClassifier = DecisionTreeClassifier(min_samples_leaf=10, max_depth=10)\n",
      "# uBoost = uBoostClassifier(uniformVariables=[X.columns[0]], neighbours=30, base_estimator=baseClassifier)\n",
      "# uBoost.fit(X, y)\n",
      "# adaBoost = AdaBoostClassifier(base_estimator=baseClassifier)\n",
      "# adaBoost.fit(X, y)\n",
      "# print adaBoost.score(X2, y2)\n",
      "# print uBoost.score(X2, y2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}